{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f33e7f4584a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "regexb=re.compile('b[\\'\\\"]')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "    \n",
    "\n",
    "from tensorflow.contrib.keras.python.keras.preprocessing import sequence\n",
    "from tensorflow.contrib.keras.python.keras.models import Sequential\n",
    "from tensorflow.contrib.keras.python.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Activation\n",
    "from tensorflow.contrib.keras.python.keras.layers.core import Reshape\n",
    "\n",
    "def loadDataCombinedColumns(path='../datasets/Combined_News_DJIA.csv'):\n",
    "    \"\"\"\n",
    "      Combine all the news headlines(25 columns) into one. \n",
    "      All the headlines for any given day is treated as one document\n",
    "      Split as per dates: 80 percent train and 20 percent test\n",
    "      replace nan with empty strings\n",
    "      make date as an index (helps with time series data)\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(path, parse_dates = True, index_col = 0, verbose =True, keep_default_na=False) \n",
    "    data_y = data[\"Label\"]\n",
    "    data_X = data.iloc[:,1:26].apply(lambda headline:cleanString(' '.join(headline)), axis=1)\n",
    "\n",
    "    \n",
    "    test_X  = data_X['2015-01-02':'2016-07-01']\n",
    "    train_X = data_X['2008-08-08':'2014-12-31']\n",
    "    test_y  = data_y['2015-01-02':'2016-07-01']\n",
    "    train_y = data_y['2008-08-08':'2014-12-31']\n",
    "    return (train_X, train_y,  test_X, test_y)\n",
    "\n",
    "\n",
    " \n",
    "def cleanString(sentence):\n",
    "    \"\"\"\n",
    "        get Grams for a  sentence\n",
    "        Custom Function\n",
    "    \"\"\"\n",
    "    return ' '.join(getGrams(sentence))\n",
    "\n",
    "\n",
    "def getGrams(sentence):\n",
    "    \"\"\"\n",
    "        get Grams for a  sentence\n",
    "        Custom Function\n",
    "    \"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    sentence = regexb.sub('', sentence)\n",
    "    sentence = regex.sub('', sentence)\n",
    "    tokens = filter(lambda token: token != '', word_tokenize(sentence))\n",
    "    #tokens = filter(lambda word: word not in stop_words, tokens)\n",
    "\n",
    "    \n",
    "    #return filter(lambda word: word not in stop_words, filter(lambda token: token != '', map(lambda token:regex.sub('', token),map(str.lower, word_tokenize(sentence)))))\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def create_dataset(dataset_X, dataset_y, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset_X)-look_back-1):\n",
    "        a = dataset_X[i:(i+look_back),: ]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset_y.iloc[i + look_back])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "\n",
    "\n",
    "train_X, train_y, test_X, test_y  = loadDataCombinedColumns()\n",
    "\n",
    "\n",
    "\n",
    "# pre built features    \n",
    "w2v_tfIdf_train_X, w2v_tf_Idf_test_X = np.loadtxt(\"../datasets/w2v_100_tfIdf_train_X.txt\"), np.loadtxt(\"../datasets/w2v_100_tfIdf_test_X.txt\")\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "  https://keras.io/getting-started/sequential-model-guide/\n",
    "  A good source to understand sequential models in Keras including lstms \n",
    "\"\"\"\n",
    "\n",
    "look_back = 5\n",
    "data_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# if return_sequences = Flase, only last LSTM cell will slpit the output.\n",
    "model.add(LSTM(32, input_shape =(look_back,data_dim), return_sequences=True ))\n",
    "\n",
    "model.add(LSTM(2))\n",
    "#model.add(Dense(10))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "time_train_X, time_train_y = create_dataset(w2v_tfIdf_train_X, train_y, look_back = look_back)\n",
    "\n",
    "time_test_X, time_test_y = create_dataset(w2v_tf_Idf_test_X, test_y, look_back = look_back)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.fit(time_train_X, time_train_y,verbose=2,epochs=10,batch_size=100, validation_data=[time_test_X, time_test_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
